{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chanyub/MusicVAE/blob/main/musicVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SowJFaiHyJaN"
      },
      "source": [
        "# 0.Setting\n",
        "\n",
        "- google Colab 환경에서 진행하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cURMdqNx6Po",
        "outputId": "2f1829f2-8018-4d71-d603-aadc59c160ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F3Pbp8O7yK8b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMkP6-TC2UdS"
      },
      "outputs": [],
      "source": [
        "!pip install magenta==2.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xvQDjth52MTB"
      },
      "outputs": [],
      "source": [
        "import magenta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct5AYFnqKP5W"
      },
      "source": [
        "# 1.Preprocessing\n",
        "\n",
        "- mid -> note_seq -> tfrecord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AH-W0-abwG0w"
      },
      "outputs": [],
      "source": [
        "data_root= '/content/drive/MyDrive/MusicVAE/groove'\n",
        "csv_file = data_root+'/info.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aVrQcKumF1Wy"
      },
      "outputs": [],
      "source": [
        "tfrec_root = '/content/drive/MyDrive/MusicVAE/tfrec/example.tfrecord'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from magenta.scripts.convert_dir_to_note_sequences import convert_directory"
      ],
      "metadata": {
        "id": "KIhvJTN_Rpip"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q_CqIRwFmY9"
      },
      "outputs": [],
      "source": [
        "convert_directory(data_root,tfrec_root,recursive=True)\n",
        "# .mid -> note_seq -> .tfrecord\n",
        "# tfrec_root 위치에 .mid파일을 .tfrecord로 변환하여 학습이 가능한 형태로 저장해줌"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGB33mJOUgZ4"
      },
      "source": [
        "## Config\n",
        "\n",
        "- 학습을 위해 모델을 포함한 Config 정의\n",
        "- train_examples_path 변수에 학습용으로 변환한 .tfrecord의 path를 넣어줌\n",
        "- Config 설정 참고: https://github.com/magenta/magenta/blob/main/magenta/models/music_vae/configs.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YmRHxnpYbxgo"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "class Config(collections.namedtuple(\n",
        "    'Config',\n",
        "    ['model', 'hparams', 'note_sequence_augmenter', 'data_converter',\n",
        "     'train_examples_path', 'eval_examples_path', 'tfds_name'])):\n",
        "\n",
        "  def values(self):\n",
        "    return self._asdict()\n",
        "\n",
        "Config.__new__.__defaults__ = (None,) * len(Config._fields)\n",
        "\n",
        "\n",
        "def update_config(config, update_dict):\n",
        "  config_dict = config.values()\n",
        "  config_dict.update(update_dict)\n",
        "  return Config(**config_dict)\n",
        "\n",
        "\n",
        "CONFIG_MAP = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Tj8NRB2JMUjK"
      },
      "outputs": [],
      "source": [
        "from magenta.common import merge_hparams\n",
        "from magenta.contrib import training as contrib_training\n",
        "from magenta.models.music_vae import MusicVAE\n",
        "from magenta.models.music_vae import lstm_models\n",
        "from magenta.models.music_vae import data\n",
        "\n",
        "HParams = contrib_training.HParams\n",
        "\n",
        "# GrooVAE configs\n",
        "CONFIG_MAP['groovae_4bar'] = Config(\n",
        "    model=MusicVAE(lstm_models.BidirectionalLstmEncoder(),\n",
        "                   lstm_models.GrooveLstmDecoder()), # MusicVAE 정의\n",
        "    hparams=merge_hparams(\n",
        "        lstm_models.get_default_hparams(),\n",
        "        HParams(\n",
        "            batch_size=512,\n",
        "            max_seq_len=16 * 4,  # 4 bars w/ 16 steps per bar\n",
        "            z_size=256,\n",
        "            enc_rnn_size=[512],\n",
        "            dec_rnn_size=[256, 256],\n",
        "            max_beta=0.2,\n",
        "            free_bits=48,\n",
        "            dropout_keep_prob=0.3,\n",
        "        )),\n",
        "    note_sequence_augmenter=None,\n",
        "    data_converter=data.GrooveConverter(\n",
        "        split_bars=4, steps_per_quarter=4, quarters_per_bar=4,\n",
        "        max_tensors_per_notesequence=20,\n",
        "        pitch_classes=data.ROLAND_DRUM_PITCH_CLASSES,\n",
        "        inference_pitch_classes=data.REDUCED_DRUM_PITCH_CLASSES),\n",
        "    # tfds_name='groove/4bar-midionly',\n",
        "    train_examples_path='/content/drive/MyDrive/MusicVAE/tfrec/example.tfrecord',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 인코더: BidirectionalLSTM\n",
        "\n",
        "- 단방향으로만 정보를 전달하던 LSTM을 개선한 양방향 LSTM\n",
        "- longer-term context 정보 전달에 좀 더 강점이 있다.\n",
        "\n",
        "### 디코더: GrooveLSTM\n",
        "\n",
        "- 베르누이분포를 기반으로 샘플링을 진행한다."
      ],
      "metadata": {
        "id": "F00MV9mekxmK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vJu1kLTSGUt"
      },
      "source": [
        "## Train Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DUoVCmFfynAy"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 The Magenta Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"MusicVAE training script.\"\"\"\n",
        "import os\n",
        "\n",
        "from magenta.models.music_vae import configs\n",
        "from magenta.models.music_vae import data\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tf_slim\n",
        "\n",
        "# Should not be called from within the graph to avoid redundant summaries.\n",
        "def _trial_summary(hparams, examples_path, output_dir):\n",
        "  \"\"\"Writes a tensorboard text summary of the trial.\"\"\"\n",
        "\n",
        "  examples_path_summary = tf.summary.text(\n",
        "      'examples_path', tf.constant(examples_path, name='examples_path'),\n",
        "      collections=[])\n",
        "\n",
        "  hparams_dict = hparams.values()\n",
        "\n",
        "  # Create a markdown table from hparams.\n",
        "  header = '| Key | Value |\\n| :--- | :--- |\\n'\n",
        "  keys = sorted(hparams_dict.keys())\n",
        "  lines = ['| %s | %s |' % (key, str(hparams_dict[key])) for key in keys]\n",
        "  hparams_table = header + '\\n'.join(lines) + '\\n'\n",
        "\n",
        "  hparam_summary = tf.summary.text(\n",
        "      'hparams', tf.constant(hparams_table, name='hparams'), collections=[])\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    writer = tf.summary.FileWriter(output_dir, graph=sess.graph)\n",
        "    writer.add_summary(examples_path_summary.eval())\n",
        "    writer.add_summary(hparam_summary.eval())\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "def _get_input_tensors(dataset, config):\n",
        "  \"\"\"Get input tensors from dataset.\"\"\"\n",
        "  batch_size = config.hparams.batch_size\n",
        "  iterator = tf.data.make_one_shot_iterator(dataset)\n",
        "  (input_sequence, output_sequence, control_sequence,\n",
        "   sequence_length) = iterator.get_next()\n",
        "  input_sequence.set_shape(\n",
        "      [batch_size, None, config.data_converter.input_depth])\n",
        "  output_sequence.set_shape(\n",
        "      [batch_size, None, config.data_converter.output_depth])\n",
        "  if not config.data_converter.control_depth:\n",
        "    control_sequence = None\n",
        "  else:\n",
        "    control_sequence.set_shape(\n",
        "        [batch_size, None, config.data_converter.control_depth])\n",
        "  sequence_length.set_shape([batch_size] + sequence_length.shape[1:].as_list())\n",
        "\n",
        "  return {\n",
        "      'input_sequence': input_sequence,\n",
        "      'output_sequence': output_sequence,\n",
        "      'control_sequence': control_sequence,\n",
        "      'sequence_length': sequence_length\n",
        "  }\n",
        "\n",
        "\n",
        "def train(train_dir,\n",
        "          config,\n",
        "          dataset_fn,\n",
        "          checkpoints_to_keep=5,\n",
        "          keep_checkpoint_every_n_hours=1,\n",
        "          num_steps=None,\n",
        "          master='',\n",
        "          num_sync_workers=0,\n",
        "          num_ps_tasks=0,\n",
        "          task=0):\n",
        "  \"\"\"Train loop.\"\"\"\n",
        "  tf.gfile.MakeDirs(train_dir)\n",
        "  is_chief = (task == 0)\n",
        "  # if is_chief:\n",
        "  #   _trial_summary(\n",
        "  #       config.hparams, config.train_examples_path or config.tfds_name,\n",
        "  #       train_dir)\n",
        "\n",
        "  with tf.Graph().as_default():\n",
        "    with tf.device(tf.train.replica_device_setter(\n",
        "        num_ps_tasks, merge_devices=True)):\n",
        "\n",
        "      model = config.model\n",
        "      model.build(config.hparams,\n",
        "                  config.data_converter.output_depth,\n",
        "                  is_training=True)\n",
        "\n",
        "      optimizer = model.train(**_get_input_tensors(dataset_fn(), config))\n",
        "\n",
        "      hooks = []\n",
        "      if num_sync_workers:\n",
        "        optimizer = tf.train.SyncReplicasOptimizer(\n",
        "            optimizer,\n",
        "            num_sync_workers)\n",
        "        hooks.append(optimizer.make_session_run_hook(is_chief))\n",
        "\n",
        "      grads, var_list = list(zip(*optimizer.compute_gradients(model.loss)))\n",
        "      global_norm = tf.global_norm(grads)\n",
        "      tf.summary.scalar('global_norm', global_norm)\n",
        "\n",
        "      if config.hparams.clip_mode == 'value':\n",
        "        g = config.hparams.grad_clip\n",
        "        clipped_grads = [tf.clip_by_value(grad, -g, g) for grad in grads]\n",
        "      elif config.hparams.clip_mode == 'global_norm':\n",
        "        clipped_grads = tf.cond(\n",
        "            global_norm < config.hparams.grad_norm_clip_to_zero,\n",
        "            lambda: tf.clip_by_global_norm(  # pylint:disable=g-long-lambda\n",
        "                grads, config.hparams.grad_clip, use_norm=global_norm)[0],\n",
        "            lambda: [tf.zeros(tf.shape(g)) for g in grads])\n",
        "      else:\n",
        "        raise ValueError(\n",
        "            'Unknown clip_mode: {}'.format(config.hparams.clip_mode))\n",
        "      train_op = optimizer.apply_gradients(\n",
        "          list(zip(clipped_grads, var_list)),\n",
        "          global_step=model.global_step,\n",
        "          name='train_step')\n",
        "\n",
        "      logging_dict = {'global_step': model.global_step,\n",
        "                      'loss': model.loss}\n",
        "\n",
        "      hooks.append(tf.train.LoggingTensorHook(logging_dict, every_n_iter=100))\n",
        "      if num_steps:\n",
        "        hooks.append(tf.train.StopAtStepHook(last_step=num_steps))\n",
        "\n",
        "      scaffold = tf.train.Scaffold(\n",
        "          saver=tf.train.Saver(\n",
        "              max_to_keep=checkpoints_to_keep,\n",
        "              keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours))\n",
        "      tf_slim.training.train(\n",
        "          train_op=train_op,\n",
        "          logdir=train_dir,\n",
        "          scaffold=scaffold,\n",
        "          hooks=hooks,\n",
        "          save_checkpoint_secs=60,\n",
        "          master=master,\n",
        "          is_chief=is_chief)\n",
        "\n",
        "\n",
        "def run(config_map,\n",
        "        tf_file_reader=tf.data.TFRecordDataset,\n",
        "        file_reader=tf.python_io.tf_record_iterator,\n",
        "        is_training=True):\n",
        "  \"\"\"Load model params, save config file and start trainer.\n",
        "\n",
        "  Args:\n",
        "    config_map: Dictionary mapping configuration name to Config object.\n",
        "    tf_file_reader: The tf.data.Dataset class to use for reading files.\n",
        "    file_reader: The Python reader to use for reading files.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if required flags are missing or invalid.\n",
        "  \"\"\"\n",
        "  config = config_map['groovae_4bar']\n",
        "  train_dir = '/content/drive/MyDrive/MusicVAE/train'\n",
        "  num_steps = None # epoch\n",
        "\n",
        "  def dataset_fn():\n",
        "    return data.get_dataset(\n",
        "        config,\n",
        "        tf_file_reader=tf_file_reader,\n",
        "        is_training=True,\n",
        "        cache_dataset=True)\n",
        "\n",
        "  if is_training == True:\n",
        "    train(\n",
        "        train_dir,\n",
        "        config=config,\n",
        "        dataset_fn=dataset_fn,\n",
        "        num_steps=num_steps)\n",
        "        # checkpoints_to_keep=FLAGS.checkpoints_to_keep,\n",
        "        # keep_checkpoint_every_n_hours=FLAGS.keep_checkpoint_every_n_hours,\n",
        "        # num_steps=FLAGS.num_steps,\n",
        "        # master=FLAGS.master,\n",
        "        # num_sync_workers=FLAGS.num_sync_workers,\n",
        "        # num_ps_tasks=FLAGS.num_ps_tasks,\n",
        "        # task=FLAGS.task)\n",
        "  else:\n",
        "    print(\"EVAL\")\n",
        "    # num_batches = FLAGS.eval_num_batches or data.count_examples(\n",
        "    #     config.eval_examples_path,\n",
        "    #     config.tfds_name,\n",
        "    #     config.data_converter,\n",
        "    #     file_reader) // config.hparams.batch_size\n",
        "    # eval_dir = os.path.join(run_dir, 'eval' + FLAGS.eval_dir_suffix)\n",
        "    # evaluate(\n",
        "    #     train_dir,\n",
        "    #     eval_dir,\n",
        "    #     config=config,\n",
        "    #     dataset_fn=dataset_fn,\n",
        "    #     num_batches=num_batches,\n",
        "    #     master=FLAGS.master)\n",
        "\n",
        "\n",
        "# def main(unused_argv):\n",
        "#   tf.logging.set_verbosity(FLAGS.log)\n",
        "#   run(configs.CONFIG_MAP)\n",
        "\n",
        "\n",
        "# def console_entry_point():\n",
        "#   tf.disable_v2_behavior()\n",
        "#   tf.app.run(main)\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#   console_entry_point()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4bBplW0EhyL"
      },
      "source": [
        "# 2. Start Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ri88cYDTRT_"
      },
      "outputs": [],
      "source": [
        "run(CONFIG_MAP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyE-Cw-MWWtH"
      },
      "source": [
        "# 3. Generate sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GrooVAE configs\n",
        "CONFIG_MAP['groovae_4bar'] = Config(\n",
        "    model=MusicVAE(lstm_models.BidirectionalLstmEncoder(),\n",
        "                   lstm_models.GrooveLstmDecoder()), # MusicVAE 정의\n",
        "    hparams=merge_hparams(\n",
        "        lstm_models.get_default_hparams(),\n",
        "        HParams(\n",
        "            batch_size=512,\n",
        "            max_seq_len=16 * 4,  # 4 bars w/ 16 steps per bar\n",
        "            z_size=256,\n",
        "            enc_rnn_size=[512],\n",
        "            dec_rnn_size=[256, 256],\n",
        "            max_beta=0.2,\n",
        "            free_bits=48,\n",
        "            dropout_keep_prob=0.3,\n",
        "        )),\n",
        "    note_sequence_augmenter=None,\n",
        "    data_converter=data.GrooveConverter(\n",
        "        split_bars=4, steps_per_quarter=4, quarters_per_bar=4,\n",
        "        max_tensors_per_notesequence=20,\n",
        "        pitch_classes=data.ROLAND_DRUM_PITCH_CLASSES,\n",
        "        inference_pitch_classes=data.REDUCED_DRUM_PITCH_CLASSES),\n",
        "    # tfds_name='groove/4bar-midionly',\n",
        "    train_examples_path='/content/drive/MyDrive/MusicVAE/tfrec/example.tfrecord',\n",
        ")"
      ],
      "metadata": {
        "id": "ZpZronGkru8l"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JTHGbIunhwV1"
      },
      "outputs": [],
      "source": [
        "from magenta.models.music_vae.trained_model import TrainedModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCVn7uomgeWZ"
      },
      "outputs": [],
      "source": [
        "model = TrainedModel(\n",
        "    config=CONFIG_MAP['groovae_4bar'],\n",
        "    batch_size=1,\n",
        "    checkpoint_dir_or_path='/content/drive/MyDrive/MusicVAE/train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pN9lcGYMfuU3"
      },
      "outputs": [],
      "source": [
        "generated_sequence = model.sample(n=1, length=16*4, temperature=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPdKHweXsojR"
      },
      "outputs": [],
      "source": [
        "generated_sequence[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KLmePiA_s5UG"
      },
      "outputs": [],
      "source": [
        "import note_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9jmUBE8KsyGn"
      },
      "outputs": [],
      "source": [
        "note_seq.sequence_proto_to_midi_file(generated_sequence[0], 'sample.mid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNitSTljfKyg"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "musicVAE.ipynb",
      "provenance": [],
      "mount_file_id": "1QyDDzqNikoON5_stEfK7lZickPPRF1uJ",
      "authorship_tag": "ABX9TyN8TFbfLstaRWxY/Wdnf/fB",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}